{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Training a Voice Recognition Model¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you havent installed fastaudio do it uncommenting and executing the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/fastaudio/fastaudio.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_basics import *\n",
    "from fastai.basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from fastaudio.core.all import *\n",
    "from fastaudio.augment.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_folder = untar_data(URLs.SPEAKERS10, extract_func=tar_extract_at_filename)\n",
    "speakers = speakers_folder.ls()\n",
    "speakers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datablock and Basic End to End Training on 10 Speakers¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop 2s from the signal and turn it to a MelSpectrogram with no augmentation\n",
    "cfg_voice = AudioConfig.Voice()\n",
    "a2s = AudioToSpec.from_cfg(cfg_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = random.choice(speakers)\n",
    "str(example).split('/')[-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks=(AudioBlock.from_folder(speakers_folder, crop_signal_to=2000), CategoryBlock),  \n",
    "                 get_items=get_audio_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 item_tfms = a2s,\n",
    "                 get_y=lambda x: str(x).split('/')[-1][:5]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cats = [y for _,y in auds.datasets(speakers_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify categories are being correctly assigned\n",
    "test_eq(min(cats).item(), 0)\n",
    "test_eq(max(cats).item(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = auds.dataloaders(speakers_folder, bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">Info:\n",
    "Show batch is fixed now on nchannels, which is an object of AudioSpectrogram (part of sg settings but we overrode getattr to make it work like an attribute).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_learner(learn, channels=1):\n",
    "    learn.model[0][0].in_channels=channels\n",
    "    learn.model[0][0].weight = torch.nn.parameter.Parameter(learn.model[0][0].weight[:,1,:,:].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "nchannels = dbunch.one_batch()[0].shape[1]\n",
    "alter_learner(learn, nchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastaudio.ci import skip_if_ci\n",
    "\n",
    "# We don't do a full lr_find when running in the CI\n",
    "@skip_if_ci\n",
    "def run_lr_find():\n",
    "    learn.lr_find()\n",
    "    \n",
    "run_lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@skip_if_ci\n",
    "def run_learner():\n",
    "    learn.fit_one_cycle(10, lr_max=slice(1e-2))\n",
    "\n",
    "# We only validate the model when running in CI\n",
    "run_learner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
