{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Speech Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you havent installed fastaudio do it uncommenting and executing the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/fastaudio/fastaudio.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_basics import *\n",
    "from fastai.basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from fastaudio.core.all import *\n",
    "from fastaudio.augment.all import *\n",
    "\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "The dataset is about 2.26 G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torchaudio.datasets.SPEECHCOMMANDS(\".\", download=True)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands_path = Path(\"SpeechCommands\")\n",
    "audio_files = get_audio_files(commands_path)\n",
    "len(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(random.choice(audio_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    f = random.choice(audio_files)\n",
    "    print(\"File:\",f )\n",
    "    print(\"Label:\", parent_label(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBMelSpec = SpectrogramTransformer(mel=True, to_db=True)\n",
    "a2s = DBMelSpec()\n",
    "crop_4000ms = ResizeSignal(4000)\n",
    "tfms = [crop_4000ms, a2s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_items=get_audio_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 item_tfms=tfms,\n",
    "                 get_y=parent_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dbunch = auds.dataloaders(commands_path, item_tfms=tfms, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to Kevin Bird and Hiromi Suenaga for these two lines to adjust a CNN model to take 1 channel input\n",
    "def alter_learner(learn, channels=1):\n",
    "    learn.model[0][0].in_channels=channels\n",
    "    learn.model[0][0].weight = torch.nn.parameter.Parameter(learn.model[0][0].weight[:,1,:,:].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(audio_dbunch, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "nchannels = audio_dbunch.one_batch()[0].shape[1]\n",
    "alter_learner(learn, nchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, lr_max=slice(1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(5, lr_max=slice(1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize our AudioToSpec Function using a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_cfg = AudioConfig.Voice()\n",
    "a2s = AudioToSpec.from_cfg(voice_cfg)\n",
    "tfms = [crop_4000ms, a2s]\n",
    "auds.item_tfms = tfms\n",
    "# tfms = Pipeline([ResizeSignal(4000),  a2s, MaskFreq(size=12), MaskTime(size=15), SGRoll()], as_item=True)\n",
    "dbunch250B = auds.dataloaders(commands_path, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch250B, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "nchannels = dbunch250B.one_batch()[0].shape[1]\n",
    "alter_learner(learn, nchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better results even without fine tuning, but much slower. We need to move a2s to the GPU and \n",
    "# then add data augmentation!\n",
    "learn.fit_one_cycle(5, lr_max=slice(2e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an MFCC with Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only grab 1500ms of the clip, voice identity can be done with shorter sections and it will speed it up\n",
    "# this is really slow for mfcc, even for 45k files, need to figure out what's going on here. Also the results\n",
    "# shouldn't be this much worse than melspectrogram\n",
    "a2mfcc = AudioToMFCC(n_mffc=20, melkwargs={\"n_fft\":2048, \"hop_length\":256, \"n_mels\":128})\n",
    "tfms = [ResizeSignal(1500), a2mfcc, Delta()]\n",
    "auds.item_tfms = tfms\n",
    "# tfms = Pipeline([ResizeSignal(4000),  a2s, MaskFreq(size=12), MaskTime(size=15), SGRoll()], as_item=True)\n",
    "dbunch_mfcc = auds.dataloaders(commands_path, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_mfcc isn't getting passed down? \n",
    "dbunch_mfcc.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch_mfcc, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, lr_max=slice(2e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>From Here:</strong><br>\n",
    "    1. Get transforms on the GPU <br>\n",
    "    2. Once it's faster test signal and spectrogram augments for speed/efficacy<br>\n",
    "    3. Fine-tune and see how high we can push results on 250 speakers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
